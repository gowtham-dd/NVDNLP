{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Data Science\\\\END to END Proj\\\\NVDNLP'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd\n",
    "os.chdir(\"../\")\n",
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "#     ENTITY: DATA TRANSFORMATION CONFIG\n",
    "# ============================================\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    train_file: Path\n",
    "    test_file: Path\n",
    "    label_encoder_file: Path\n",
    "    tfidf_vectorizer_file: Path\n",
    "    test_size: float\n",
    "    random_state: int\n",
    "    max_features: int\n",
    "    ngram_range: tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ⚙️ CONFIGURATION MANAGER\n",
    "# ============================================\n",
    "\n",
    "from src.NVDNLP.utils.common import read_yaml, create_directories \n",
    "# from src.NVDNLP.entity import DataTransformationConfig\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = \"config/config.yaml\",\n",
    "        params_filepath = \"params.yaml\",\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "        params = self.params.TFIDF\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            data_path=Path(config.data_path),\n",
    "            train_file=Path(config.train_file),\n",
    "            test_file=Path(config.test_file),\n",
    "            label_encoder_file=Path(config.label_encoder_file),\n",
    "            tfidf_vectorizer_file=Path(config.tfidf_vectorizer_file),\n",
    "            test_size=self.params.training.test_size,\n",
    "            random_state=self.params.training.random_state,\n",
    "            max_features=params.max_features,\n",
    "            ngram_range=tuple(params.ngram_range),\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 🔄 DATA TRANSFORMATION COMPONENT\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from src.NVDNLP.entity.config_entity import DataTransformationConfig\n",
    "from src.NVDNLP import logger\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        self.label_encoder = None\n",
    "        self.tfidf_vectorizer = None\n",
    "\n",
    "    def is_transformation_completed(self) -> bool:\n",
    "        \"\"\"Check if data transformation has already been completed by looking for output files\"\"\"\n",
    "        try:\n",
    "            required_files = [\n",
    "                self.config.label_encoder_file,\n",
    "                self.config.tfidf_vectorizer_file,\n",
    "                self.config.train_file,\n",
    "                self.config.test_file\n",
    "            ]\n",
    "            \n",
    "            # Check if all required files exist\n",
    "            all_files_exist = all(os.path.exists(file) for file in required_files)\n",
    "            \n",
    "            if all_files_exist:\n",
    "                logger.info(\" Data transformation already completed successfully. Skipping...\")\n",
    "                \n",
    "                # Verify files can be loaded properly\n",
    "                try:\n",
    "                    # Test load one file to ensure it's valid\n",
    "                    test_encoder = joblib.load(self.config.label_encoder_file)\n",
    "                    logger.info(\" Transformation artifacts verified and valid\")\n",
    "                    return True\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\" Existing transformation files corrupted. Re-running transformation...\")\n",
    "                    return False\n",
    "            else:\n",
    "                logger.info(\" Transformation artifacts not found. Starting transformation...\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\" Error checking transformation status: {e}\")\n",
    "            return False\n",
    "\n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load the cleaned dataset\"\"\"\n",
    "        try:\n",
    "            logger.info(\" Loading dataset for transformation...\")\n",
    "            df = pd.read_csv(self.config.data_path)\n",
    "            logger.info(f\" Dataset loaded. Total rows: {len(df)}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.info(f\" Failed to load dataset: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def encode_labels(self, severity_series: pd.Series) -> np.ndarray:\n",
    "        \"\"\"Encode categorical severity labels to numerical values\"\"\"\n",
    "        try:\n",
    "            logger.info(\" Encoding severity labels...\")\n",
    "            self.label_encoder = LabelEncoder()\n",
    "            y_encoded = self.label_encoder.fit_transform(severity_series)\n",
    "            logger.info(f\" Encoded severity classes: {list(self.label_encoder.classes_)}\")\n",
    "            return y_encoded\n",
    "        except Exception as e:\n",
    "            logger.info(f\" Label encoding failed: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def generate_tfidf_features(self, descriptions: pd.Series):\n",
    "        \"\"\"Generate TF-IDF features from text descriptions\"\"\"\n",
    "        try:\n",
    "            logger.info(\" Generating enhanced TF-IDF features...\")\n",
    "            self.tfidf_vectorizer = TfidfVectorizer(\n",
    "                max_features=self.config.max_features,\n",
    "                ngram_range=self.config.ngram_range,\n",
    "                stop_words='english'\n",
    "            )\n",
    "            X_tfidf = self.tfidf_vectorizer.fit_transform(descriptions.astype(str))\n",
    "            logger.info(f\" TF-IDF shape: {X_tfidf.shape}\")\n",
    "            return X_tfidf\n",
    "        except Exception as e:\n",
    "            logger.error(f\" TF-IDF feature generation failed: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def split_data(self, df: pd.DataFrame, y_encoded: np.ndarray):\n",
    "        \"\"\"Split data into training and testing sets and return DataFrames\"\"\"\n",
    "        try:\n",
    "            logger.info(\" Splitting data into train and test sets...\")\n",
    "            \n",
    "            # Split indices\n",
    "            train_indices, test_indices = train_test_split(\n",
    "                df.index,\n",
    "                test_size=self.config.test_size,\n",
    "                random_state=self.config.random_state,\n",
    "                stratify=y_encoded\n",
    "            )\n",
    "            \n",
    "            # Create train and test DataFrames\n",
    "            train_df = df.loc[train_indices].copy()\n",
    "            test_df = df.loc[test_indices].copy()\n",
    "            \n",
    "            # Add encoded labels to DataFrames\n",
    "            train_df['encoded_severity'] = y_encoded[train_indices]\n",
    "            test_df['encoded_severity'] = y_encoded[test_indices]\n",
    "            \n",
    "            logger.info(f\" Train size: {len(train_df)}, Test size: {len(test_df)}\")\n",
    "            return train_df, test_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\" Data splitting failed: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def save_artifacts(self, train_df: pd.DataFrame, test_df: pd.DataFrame):\n",
    "        \"\"\"Save transformed data and transformers\"\"\"\n",
    "        try:\n",
    "            logger.info(\" Saving transformation artifacts...\")\n",
    "            \n",
    "            # Create directory if it doesn't exist\n",
    "            os.makedirs(self.config.root_dir, exist_ok=True)\n",
    "            \n",
    "            # Save label encoder as PKL\n",
    "            joblib.dump(self.label_encoder, self.config.label_encoder_file)\n",
    "            logger.info(f\" Saved label encoder: {self.config.label_encoder_file}\")\n",
    "            \n",
    "            # Save TF-IDF vectorizer as PKL\n",
    "            joblib.dump(self.tfidf_vectorizer, self.config.tfidf_vectorizer_file)\n",
    "            logger.info(f\" Saved TF-IDF vectorizer: {self.config.tfidf_vectorizer_file}\")\n",
    "            \n",
    "            # Save train and test as CSV files\n",
    "            train_df.to_csv(self.config.train_file, index=False)\n",
    "            test_df.to_csv(self.config.test_file, index=False)\n",
    "            \n",
    "            logger.info(f\" Saved train data (CSV): {self.config.train_file}\")\n",
    "            logger.info(f\" Saved test data (CSV): {self.config.test_file}\")\n",
    "            \n",
    "            # Create a status file to mark completion\n",
    "            status_file = os.path.join(self.config.root_dir, \"transformation_status.txt\")\n",
    "            with open(status_file, 'w') as f:\n",
    "                f.write(\"Data Transformation Status: COMPLETED\\n\")\n",
    "                f.write(f\"Label Encoder: {self.config.label_encoder_file}\\n\")\n",
    "                f.write(f\"TF-IDF Vectorizer: {self.config.tfidf_vectorizer_file}\\n\")\n",
    "                f.write(f\"Train Data: {self.config.train_file}\\n\")\n",
    "                f.write(f\"Test Data: {self.config.test_file}\\n\")\n",
    "                f.write(f\"Train Samples: {len(train_df)}\\n\")\n",
    "                f.write(f\"Test Samples: {len(test_df)}\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\" Failed to save artifacts: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def load_existing_artifacts(self):\n",
    "        \"\"\"Load existing transformation artifacts\"\"\"\n",
    "        try:\n",
    "            logger.info(\" Loading existing transformation artifacts...\")\n",
    "            \n",
    "            # Load transformers (PKL files)\n",
    "            self.label_encoder = joblib.load(self.config.label_encoder_file)\n",
    "            self.tfidf_vectorizer = joblib.load(self.config.tfidf_vectorizer_file)\n",
    "            \n",
    "            # Load train and test data (CSV files)\n",
    "            train_df = pd.read_csv(self.config.train_file)\n",
    "            test_df = pd.read_csv(self.config.test_file)\n",
    "            \n",
    "            logger.info(\" Successfully loaded existing transformation artifacts\")\n",
    "            \n",
    "            return {\n",
    "                'train_df': train_df,\n",
    "                'test_df': test_df,\n",
    "                'label_encoder': self.label_encoder,\n",
    "                'tfidf_vectorizer': self.tfidf_vectorizer\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\" Failed to load existing artifacts: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def transform(self):\n",
    "        \"\"\"Perform complete data transformation pipeline only if not already completed\"\"\"\n",
    "        try:\n",
    "            # Check if transformation is already completed\n",
    "            if self.is_transformation_completed():\n",
    "                return self.load_existing_artifacts()\n",
    "            \n",
    "            logger.info(\" Starting Data Transformation Pipeline...\")\n",
    "            \n",
    "            # Step 1: Load data\n",
    "            df = self.load_data()\n",
    "            \n",
    "            # Step 2: Prepare features and target\n",
    "            X = df[\"Description\"].astype(str)\n",
    "            y = df[\"Severity\"]\n",
    "            \n",
    "            # Step 3: Encode labels\n",
    "            y_encoded = self.encode_labels(y)\n",
    "            \n",
    "            # Step 4: Generate TF-IDF features\n",
    "            X_tfidf = self.generate_tfidf_features(X)\n",
    "            \n",
    "            # Step 5: Split data into train and test DataFrames\n",
    "            train_df, test_df = self.split_data(df, y_encoded)\n",
    "            \n",
    "            # Step 6: Save artifacts\n",
    "            self.save_artifacts(train_df, test_df)\n",
    "            \n",
    "            logger.info(\" Data Transformation completed successfully!\")\n",
    "            \n",
    "            return {\n",
    "                'train_df': train_df,\n",
    "                'test_df': test_df,\n",
    "                'label_encoder': self.label_encoder,\n",
    "                'tfidf_vectorizer': self.tfidf_vectorizer,\n",
    "                'X_tfidf': X_tfidf,\n",
    "                'y_encoded': y_encoded\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\" Data transformation failed: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-22 21:07:26,761: INFO: 4190284576: >>>>>> Stage Data Transformation stage started <<<<<<]\n",
      "[2025-10-22 21:07:26,771: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-10-22 21:07:26,775: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-10-22 21:07:26,778: INFO: common: created directory at: artifacts]\n",
      "[2025-10-22 21:07:26,780: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2025-10-22 21:07:26,782: INFO: 2947922453:  Data transformation already completed successfully. Skipping...]\n",
      "[2025-10-22 21:07:26,910: INFO: 2947922453:  Transformation artifacts verified and valid]\n",
      "[2025-10-22 21:07:26,912: INFO: 2947922453:  Loading existing transformation artifacts...]\n",
      "[2025-10-22 21:07:30,683: INFO: 2947922453:  Successfully loaded existing transformation artifacts]\n",
      "[2025-10-22 21:07:30,808: INFO: 4190284576: >>>>>> Stage Data Transformation stage completed <<<<<<\n",
      "\n",
      "x==========x]\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 🔄 DATA TRANSFORMATION PIPELINE\n",
    "# ============================================\n",
    "\n",
    "from src.NVDNLP.config.configuration import ConfigurationManager\n",
    "# from src.NVDNLP.components.DataTransformation import DataTransformation\n",
    "from src.NVDNLP import logger\n",
    "\n",
    "STAGE_NAME = \"Data Transformation stage\"\n",
    "\n",
    "class DataTransformationTrainingPipeline:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def main(self):\n",
    "        config = ConfigurationManager()\n",
    "        data_transformation_config = config.get_data_transformation_config()\n",
    "        data_transformation = DataTransformation(config=data_transformation_config)\n",
    "        \n",
    "        # This will automatically skip if already completed\n",
    "        transformed_data = data_transformation.transform()\n",
    "        \n",
    "        return transformed_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        logger.info(f\">>>>>> Stage {STAGE_NAME} started <<<<<<\")\n",
    "        obj = DataTransformationTrainingPipeline()\n",
    "        result = obj.main()\n",
    "        \n",
    "        if result:\n",
    "            logger.info(f\">>>>>> Stage {STAGE_NAME} completed <<<<<<\\n\\nx==========x\")\n",
    "        else:\n",
    "            logger.info(f\">>>>>> Stage {STAGE_NAME} skipped (already completed) <<<<<<\\n\\nx==========x\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.exception(e)\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
