{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Data Science\\\\END to END Proj\\\\NVDNLP'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd\n",
    "os.chdir(\"../\")\n",
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "#     ENTITY: DATA VALIDATION CONFIG\n",
    "# ============================================\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataValidationConfig:\n",
    "    root_dir: Path\n",
    "    STATUS_FILE: str\n",
    "    ALL_REQUIRED_FILES: list\n",
    "    REQUIRED_COLUMNS: list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ⚙️ CONFIGURATION MANAGER\n",
    "# ============================================\n",
    "from src.NVDNLP.constant import *\n",
    "from src.NVDNLP.utils.common import read_yaml, create_directories \n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_data_validation_config(self) -> DataValidationConfig:\n",
    "        config = self.config.data_validation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_validation_config = DataValidationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            STATUS_FILE=config.STATUS_FILE,\n",
    "            ALL_REQUIRED_FILES=config.ALL_REQUIRED_FILES,\n",
    "            REQUIRED_COLUMNS=config.REQUIRED_COLUMNS,\n",
    "        )\n",
    "\n",
    "        return data_validation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 🔍 DATA VALIDATION COMPONENT\n",
    "# ============================================\n",
    "import os\n",
    "import pandas as pd\n",
    "from NVDNLP import logger\n",
    "\n",
    "class DataValidation:\n",
    "    def __init__(self, config: DataValidationConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def is_validation_completed(self) -> bool:\n",
    "        \"\"\"Check if validation has already been completed by looking for status file\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.config.STATUS_FILE):\n",
    "                with open(self.config.STATUS_FILE, 'r') as f:\n",
    "                    status_content = f.read().strip()\n",
    "                    if \"Validation status: True\" in status_content:\n",
    "                        logger.info(\" Data validation already completed successfully. Skipping...\")\n",
    "                        return True\n",
    "                    elif \"Validation status: False\" in status_content:\n",
    "                        logger.info(\"Previous validation failed. Re-running validation...\")\n",
    "                        return False\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Error checking validation status: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def validate_all_files_exist(self) -> bool:\n",
    "        \"\"\"Validate that all required files exist\"\"\"\n",
    "        try:\n",
    "            validation_status = True\n",
    "            \n",
    "            # Check if all required files exist\n",
    "            for file in self.config.ALL_REQUIRED_FILES:\n",
    "                file_path = os.path.join(\"artifacts\", \"data_ingestion\", file)\n",
    "                if not os.path.exists(file_path):\n",
    "                    validation_status = False\n",
    "                    logger.info(f\"Missing required file: {file}\")\n",
    "                    break\n",
    "            \n",
    "            # Write validation status to file\n",
    "            with open(self.config.STATUS_FILE, 'w') as f:\n",
    "                f.write(f\"Validation status: {validation_status}\")\n",
    "            \n",
    "            return validation_status\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.info(f\"Error in file validation: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def validate_dataset_columns(self, data_path: str) -> bool:\n",
    "        \"\"\"Validate that dataset contains all required columns\"\"\"\n",
    "        try:\n",
    "            validation_status = True\n",
    "            \n",
    "            # Read the dataset\n",
    "            df = pd.read_csv(data_path)\n",
    "            \n",
    "            # Check if all required columns are present\n",
    "            missing_columns = []\n",
    "            for column in self.config.REQUIRED_COLUMNS:\n",
    "                if column not in df.columns:\n",
    "                    missing_columns.append(column)\n",
    "                    validation_status = False\n",
    "            \n",
    "            if missing_columns:\n",
    "                logger.info(f\"Missing required columns: {missing_columns}\")\n",
    "            else:\n",
    "                logger.info(\" All required columns are present!\")\n",
    "                \n",
    "                # Additional validation: Check for null values in critical columns\n",
    "                critical_columns = ['Description', 'Severity']\n",
    "                null_check_status = True\n",
    "                for col in critical_columns:\n",
    "                    null_count = df[col].isnull().sum()\n",
    "                    if null_count > 0:\n",
    "                        logger.info(f\"Column '{col}' has {null_count} null values\")\n",
    "                        null_check_status = False\n",
    "                \n",
    "                if null_check_status:\n",
    "                    logger.info(\" No null values in critical columns!\")\n",
    "                else:\n",
    "                    logger.info(\"  Null values found in critical columns\")\n",
    "            \n",
    "            # Write detailed validation report\n",
    "            validation_report = {\n",
    "                'validation_status': validation_status,\n",
    "                'missing_columns': missing_columns,\n",
    "                'total_rows': len(df),\n",
    "                'columns_present': list(df.columns),\n",
    "                'severity_distribution': df['Severity'].value_counts().to_dict() if 'Severity' in df.columns else {}\n",
    "            }\n",
    "            \n",
    "            # Save validation report\n",
    "            report_path = os.path.join(self.config.root_dir, \"validation_report.txt\")\n",
    "            with open(report_path, 'w') as f:\n",
    "                f.write(\"=== DATA VALIDATION REPORT ===\\n\")\n",
    "                f.write(f\"Overall Status: {'PASS' if validation_status else 'FAIL'}\\n\")\n",
    "                f.write(f\"Total Rows: {validation_report['total_rows']}\\n\")\n",
    "                f.write(f\"Missing Columns: {missing_columns}\\n\")\n",
    "                f.write(f\"Columns Present: {validation_report['columns_present']}\\n\")\n",
    "                f.write(f\"Severity Distribution: {validation_report['severity_distribution']}\\n\")\n",
    "            \n",
    "            # Update status file\n",
    "            with open(self.config.STATUS_FILE, 'w') as f:\n",
    "                f.write(f\"Validation status: {validation_status}\")\n",
    "            \n",
    "            return validation_status\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.info(f\"Error in dataset validation: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def validate_all(self, data_path: str) -> bool:\n",
    "        \"\"\"Perform complete data validation only if not already completed\"\"\"\n",
    "        try:\n",
    "            # Check if validation is already completed\n",
    "            if self.is_validation_completed():\n",
    "                return True\n",
    "            \n",
    "            logger.info(\" Starting Data Validation Process...\")\n",
    "            \n",
    "            # Validate files exist\n",
    "            files_valid = self.validate_all_files_exist()\n",
    "            \n",
    "            # Validate dataset structure\n",
    "            dataset_valid = self.validate_dataset_columns(data_path)\n",
    "            \n",
    "            # Overall validation status\n",
    "            overall_status = files_valid and dataset_valid\n",
    "            \n",
    "            if overall_status:\n",
    "                logger.info(\" Data Validation PASSED - All checks completed successfully!\")\n",
    "            else:\n",
    "                logger.info(\" Data Validation FAILED - Check validation report for details!\")\n",
    "            \n",
    "            return overall_status\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.info(f\"Error in complete validation: {e}\")\n",
    "            # Mark as failed in status file\n",
    "            with open(self.config.STATUS_FILE, 'w') as f:\n",
    "                f.write(\"Validation status: False\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-22 19:51:10,312: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-10-22 19:51:10,315: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-10-22 19:51:10,316: INFO: common: created directory at: artifacts]\n",
      "[2025-10-22 19:51:10,319: INFO: common: created directory at: artifacts/data_validation]\n",
      "[2025-10-22 19:51:10,350: INFO: 3267598641: Previous validation failed. Re-running validation...]\n",
      "[2025-10-22 19:51:10,350: INFO: 3267598641: Previous validation failed. Re-running validation...]\n",
      "[2025-10-22 19:51:10,356: INFO: 3267598641:  Starting Data Validation Process...]\n",
      "[2025-10-22 19:51:13,345: INFO: 3267598641:  All required columns are present!]\n",
      "[2025-10-22 19:51:13,454: INFO: 3267598641: Column 'Severity' has 13516 null values]\n",
      "[2025-10-22 19:51:13,456: INFO: 3267598641:   Null values found in critical columns]\n",
      "[2025-10-22 19:51:13,623: INFO: 3267598641:  Data Validation PASSED - All checks completed successfully!]\n",
      "[2025-10-22 19:51:13,626: INFO: 3978983681:  Data Validation Completed Successfully!]\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 🔄 DATA VALIDATION PIPELINE\n",
    "# ============================================\n",
    "\n",
    "from NVDNLP import logger\n",
    "\n",
    "try:\n",
    "    # Initialize configuration\n",
    "    config = ConfigurationManager()\n",
    "    data_validation_config = config.get_data_validation_config()\n",
    "    \n",
    "    # Initialize data validation\n",
    "    data_validation = DataValidation(config=data_validation_config)\n",
    "    \n",
    "    # Check if validation is already completed\n",
    "    if data_validation.is_validation_completed():\n",
    "        \n",
    "        logger.info(\"  Data validation already completed. Moving to next stage...\")\n",
    "    else:\n",
    "        # Perform validation\n",
    "        data_path = \"artifacts/data_ingestion/nvd_combined_2010_2025.csv\"\n",
    "        validation_status = data_validation.validate_all(data_path)\n",
    "        \n",
    "        if validation_status:\n",
    "            logger.info(\" Data Validation Completed Successfully!\")\n",
    "        else:\n",
    "            logger.info(\" Data Validation Failed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.info(f\"Data Validation Pipeline Failed: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
