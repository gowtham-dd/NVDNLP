{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Data Science\\\\END to END Proj\\\\NVDNLP'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd\n",
    "os.chdir(\"../\")\n",
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "#     ENTITY: MODEL EVALUATION CONFIG\n",
    "# ============================================\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    test_data_path: Path\n",
    "    model_path: Path\n",
    "    label_encoder_path: Path\n",
    "    tfidf_vectorizer_path: Path\n",
    "    metric_file_name: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ⚙️ CONFIGURATION MANAGER\n",
    "# ============================================\n",
    "\n",
    "from src.NVDNLP.utils.common import read_yaml, create_directories \n",
    "# from src.NVDNLP.entity.config_entity import ModelEvaluationConfig\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = \"config/config.yaml\",\n",
    "        params_filepath = \"params.yaml\",\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            test_data_path=Path(config.test_data_path),\n",
    "            model_path=Path(config.model_path),\n",
    "            label_encoder_path=Path(config.label_encoder_path),\n",
    "            tfidf_vectorizer_path=Path(config.tfidf_vectorizer_path),\n",
    "            metric_file_name=Path(config.metric_file_name)\n",
    "        )\n",
    "\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "#     MODEL EVALUATION COMPONENT (FIXED FOR CLASS MISMATCH)\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from src.NVDNLP.entity.config_entity import ModelEvaluationConfig\n",
    "from src.NVDNLP import logger\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.label_encoder = None\n",
    "        self.tfidf_vectorizer = None\n",
    "\n",
    "    def load_artifacts(self):\n",
    "        \"\"\"Load model, label encoder, and TF-IDF vectorizer\"\"\"\n",
    "        try:\n",
    "            logger.info(\"     Loading evaluation artifacts...\")\n",
    "            \n",
    "            # Load trained model\n",
    "            self.model = joblib.load(self.config.model_path)\n",
    "            logger.info(f\"  Model loaded: {self.config.model_path}\")\n",
    "            \n",
    "            # Load label encoder\n",
    "            self.label_encoder = joblib.load(self.config.label_encoder_path)\n",
    "            logger.info(f\"  Label encoder loaded: {self.config.label_encoder_path}\")\n",
    "            logger.info(f\"     Label encoder classes: {list(self.label_encoder.classes_)}\")\n",
    "            \n",
    "            # Load TF-IDF vectorizer\n",
    "            self.tfidf_vectorizer = joblib.load(self.config.tfidf_vectorizer_path)\n",
    "            logger.info(f\"  TF-IDF vectorizer loaded: {self.config.tfidf_vectorizer_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"   Failed to load artifacts: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def load_test_data(self):\n",
    "        \"\"\"Load and prepare test data - filter to only known classes\"\"\"\n",
    "        try:\n",
    "            logger.info(\"     Loading test data...\")\n",
    "            \n",
    "            # Load test CSV file\n",
    "            test_df = pd.read_csv(self.config.test_data_path)\n",
    "            logger.info(f\"  Test data loaded: {len(test_df)} samples\")\n",
    "            \n",
    "            # Get known classes from label encoder\n",
    "            known_classes = list(self.label_encoder.classes_)\n",
    "            logger.info(f\"     Known severity classes: {known_classes}\")\n",
    "            \n",
    "            # Check for unknown classes in test data\n",
    "            unique_test_severities = test_df['Severity'].unique()\n",
    "            logger.info(f\"     All severities in test data: {list(unique_test_severities)}\")\n",
    "            \n",
    "            # Filter test data to only include known classes\n",
    "            original_size = len(test_df)\n",
    "            test_df_filtered = test_df[test_df['Severity'].isin(known_classes)].copy()\n",
    "            filtered_size = len(test_df_filtered)\n",
    "            \n",
    "            if original_size != filtered_size:\n",
    "                logger.warning(f\"     Filtered out {original_size - filtered_size} samples with unknown classes\")\n",
    "                logger.info(f\"    Using {filtered_size} samples for evaluation\")\n",
    "            \n",
    "            # Check encoded severity distribution\n",
    "            encoded_severity_counts = test_df_filtered['encoded_severity'].value_counts().sort_index()\n",
    "            logger.info(f\"    Encoded severity distribution: {dict(encoded_severity_counts)}\")\n",
    "            \n",
    "            # Prepare features and labels\n",
    "            X_test_descriptions = test_df_filtered['Description'].astype(str)\n",
    "            y_test_encoded = test_df_filtered['encoded_severity']\n",
    "            \n",
    "            # Transform descriptions to TF-IDF features\n",
    "            X_test_tfidf = self.tfidf_vectorizer.transform(X_test_descriptions)\n",
    "            logger.info(f\"  Test features transformed: {X_test_tfidf.shape}\")\n",
    "            \n",
    "            return X_test_tfidf, y_test_encoded, test_df_filtered\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"   Failed to load test data: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def make_predictions(self, X_test):\n",
    "        \"\"\"Make predictions on test data\"\"\"\n",
    "        try:\n",
    "            logger.info(\"    Making predictions on test data...\")\n",
    "            \n",
    "            y_pred = self.model.predict(X_test)\n",
    "            logger.info(f\"  Predictions completed: {len(y_pred)} predictions\")\n",
    "            \n",
    "            # Check unique values in predictions\n",
    "            unique_predictions = np.unique(y_pred)\n",
    "            logger.info(f\"    Unique predicted classes: {list(unique_predictions)}\")\n",
    "            \n",
    "            return y_pred\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"   Prediction failed: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def calculate_metrics(self, y_test, y_pred):\n",
    "        \"\"\"Calculate evaluation metrics using only known classes\"\"\"\n",
    "        try:\n",
    "            logger.info(\"    Calculating evaluation metrics...\")\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            # Generate classification report using only known classes\n",
    "            class_report = classification_report(\n",
    "                y_test, \n",
    "                y_pred, \n",
    "                target_names=self.label_encoder.classes_,\n",
    "                output_dict=True,\n",
    "                zero_division=0\n",
    "            )\n",
    "            \n",
    "            # Generate confusion matrix\n",
    "            conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "            \n",
    "            # Convert confusion matrix to list for JSON serialization\n",
    "            conf_matrix_list = conf_matrix.tolist()\n",
    "            \n",
    "            logger.info(f\"    Accuracy: {accuracy*100:.2f}%\")\n",
    "            \n",
    "            return {\n",
    "                'accuracy': accuracy,\n",
    "                'classification_report': class_report,\n",
    "                'confusion_matrix': conf_matrix_list,\n",
    "                'severity_classes': list(self.label_encoder.classes_),\n",
    "                'test_samples': len(y_test)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"   Metric calculation failed: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def save_metrics(self, metrics):\n",
    "        \"\"\"Save evaluation metrics to file\"\"\"\n",
    "        try:\n",
    "            logger.info(\"    Saving evaluation metrics...\")\n",
    "            \n",
    "            # Create detailed metrics report\n",
    "            metrics_report = {\n",
    "                'overall_accuracy': metrics['accuracy'],\n",
    "                'test_samples': metrics['test_samples'],\n",
    "                'severity_classes': metrics['severity_classes'],\n",
    "                'confusion_matrix': metrics['confusion_matrix'],\n",
    "                'detailed_classification_report': metrics['classification_report']\n",
    "            }\n",
    "            \n",
    "            # Save as CSV for easy viewing\n",
    "            csv_metrics = {\n",
    "                'metric': ['overall_accuracy', 'test_samples'],\n",
    "                'value': [metrics['accuracy'], metrics['test_samples']]\n",
    "            }\n",
    "            \n",
    "            # Add per-class metrics\n",
    "            for class_name in metrics['severity_classes']:\n",
    "                if class_name in metrics['classification_report']:\n",
    "                    class_metrics = metrics['classification_report'][class_name]\n",
    "                    csv_metrics['metric'].extend([\n",
    "                        f'{class_name}_precision',\n",
    "                        f'{class_name}_recall', \n",
    "                        f'{class_name}_f1_score',\n",
    "                        f'{class_name}_support'\n",
    "                    ])\n",
    "                    csv_metrics['value'].extend([\n",
    "                        class_metrics['precision'],\n",
    "                        class_metrics['recall'],\n",
    "                        class_metrics['f1-score'],\n",
    "                        class_metrics['support']\n",
    "                    ])\n",
    "            \n",
    "            # Create DataFrame and save as CSV\n",
    "            metrics_df = pd.DataFrame(csv_metrics)\n",
    "            metrics_df.to_csv(self.config.metric_file_name, index=False)\n",
    "            logger.info(f\"  Metrics saved to: {self.config.metric_file_name}\")\n",
    "            \n",
    "            # Save detailed report as JSON\n",
    "            import json\n",
    "            detailed_metrics_file = self.config.metric_file_name.with_suffix('.json')\n",
    "            with open(detailed_metrics_file, 'w') as f:\n",
    "                json.dump(metrics_report, f, indent=4)\n",
    "            logger.info(f\"  Detailed metrics saved to: {detailed_metrics_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"   Failed to save metrics: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def print_evaluation_summary(self, metrics, y_test, y_pred):\n",
    "        \"\"\"Print comprehensive evaluation summary\"\"\"\n",
    "        try:\n",
    "            logger.info(\"\\n\" + \"=\"*60)\n",
    "            logger.info(\"    MODEL EVALUATION SUMMARY\")\n",
    "            logger.info(\"=\"*60)\n",
    "            logger.info(f\"    Overall Accuracy: {metrics['accuracy']*100:.2f}%\")\n",
    "            logger.info(f\"    Test Samples: {metrics['test_samples']}\")\n",
    "            logger.info(f\" Severity Classes: {', '.join(metrics['severity_classes'])}\")\n",
    "            \n",
    "            logger.info(\"\\n Classification Report:\")\n",
    "            clean_report = classification_report(\n",
    "                y_test, \n",
    "                y_pred, \n",
    "                target_names=metrics['severity_classes'],\n",
    "                zero_division=0\n",
    "            )\n",
    "            logger.info(clean_report)\n",
    "            \n",
    "            logger.info(\"\\n Confusion Matrix:\")\n",
    "            logger.info(\"Rows: Actual, Columns: Predicted\")\n",
    "            logger.info(f\"Labels: {metrics['severity_classes']}\")\n",
    "            logger.info(np.array2string(\n",
    "                np.array(metrics['confusion_matrix']), \n",
    "                formatter={'int': lambda x: f'{x:6d}'}\n",
    "            ))\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"   Failed to print evaluation summary: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def debug_data_issues(self):\n",
    "        \"\"\"Debug method to identify data issues\"\"\"\n",
    "        try:\n",
    "            logger.info(\" DEBUG: Data Issue Analysis\")\n",
    "            \n",
    "            # Load full test data\n",
    "            test_df = pd.read_csv(self.config.test_data_path)\n",
    "            \n",
    "            # Check severity distribution\n",
    "            severity_counts = test_df['Severity'].value_counts()\n",
    "            logger.info(f\"    Full test data severity distribution:\\n{severity_counts}\")\n",
    "            \n",
    "            # Check for unknown classes\n",
    "            known_classes = list(self.label_encoder.classes_)\n",
    "            unknown_classes = set(test_df['Severity'].unique()) - set(known_classes)\n",
    "            \n",
    "            if unknown_classes:\n",
    "                logger.warning(f\" Unknown severity classes found: {list(unknown_classes)}\")\n",
    "                logger.warning(f\"    Samples with unknown classes: {len(test_df[test_df['Severity'].isin(unknown_classes)])}\")\n",
    "            \n",
    "            # Check encoded values\n",
    "            logger.info(f\"    Encoded values range: {test_df['encoded_severity'].min()} to {test_df['encoded_severity'].max()}\")\n",
    "            \n",
    "            return len(unknown_classes) > 0\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"   Debug analysis failed: {e}\")\n",
    "            return False\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Complete model evaluation pipeline\"\"\"\n",
    "        try:\n",
    "            # Check if metrics file already exists\n",
    "            if self.config.metric_file_name.exists():\n",
    "                logger.info(\"  Metrics file already exists. Skipping evaluation...\")\n",
    "                return {\n",
    "                    'status': 'skipped',\n",
    "                    'message': 'Evaluation already completed',\n",
    "                    'metrics_file': self.config.metric_file_name\n",
    "                }\n",
    "            \n",
    "            logger.info(\" Starting Model Evaluation Pipeline...\")\n",
    "            \n",
    "            # Step 1: Load artifacts\n",
    "            self.load_artifacts()\n",
    "            \n",
    "            # Step 2: Debug data issues\n",
    "            has_unknown_classes = self.debug_data_issues()\n",
    "            \n",
    "            # Step 3: Load and prepare test data (filters unknown classes)\n",
    "            X_test, y_test, test_df = self.load_test_data()\n",
    "            \n",
    "            # Step 4: Make predictions\n",
    "            y_pred = self.make_predictions(X_test)\n",
    "            \n",
    "            # Step 5: Calculate metrics\n",
    "            metrics = self.calculate_metrics(y_test, y_pred)\n",
    "            \n",
    "            # Step 6: Save metrics\n",
    "            self.save_metrics(metrics)\n",
    "            \n",
    "            # Step 7: Print summary\n",
    "            self.print_evaluation_summary(metrics, y_test, y_pred)\n",
    "            \n",
    "            if has_unknown_classes:\n",
    "                logger.warning(\" Evaluation completed with filtered data (unknown classes removed)\")\n",
    "            else:\n",
    "                logger.info(\"  Model Evaluation completed successfully!\")\n",
    "            \n",
    "            return {\n",
    "                'status': 'completed',\n",
    "                'message': 'Evaluation completed successfully',\n",
    "                'accuracy': metrics['accuracy'],\n",
    "                'metrics_file': self.config.metric_file_name,\n",
    "                'test_samples': metrics['test_samples'],\n",
    "                'severity_classes': metrics['severity_classes'],\n",
    "                'has_unknown_classes': has_unknown_classes\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"   Model evaluation pipeline failed: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-22 23:07:01,184: INFO: 390888635: >>>>>> Stage Model Evaluation stage started <<<<<<]\n",
      "[2025-10-22 23:07:01,192: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-10-22 23:07:01,198: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-10-22 23:07:01,200: INFO: common: created directory at: artifacts]\n",
      "[2025-10-22 23:07:01,202: INFO: common: created directory at: artifacts/model_evaluation]\n",
      "[2025-10-22 23:07:01,204: INFO: 3855641508:   Metrics file already exists. Skipping evaluation...]\n",
      "[2025-10-22 23:07:01,207: INFO: 390888635: >>>>>> Stage Model Evaluation stage skipped (evaluation already completed) <<<<<<\n",
      "\n",
      "x==========x]\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "#     MODEL EVALUATION PIPELINE\n",
    "# ============================================\n",
    "\n",
    "from src.NVDNLP.config.configuration import ConfigurationManager\n",
    "# from src.NVDNLP.components.ModelEvaluation import ModelEvaluation\n",
    "from src.NVDNLP import logger\n",
    "\n",
    "STAGE_NAME = \"Model Evaluation stage\"\n",
    "\n",
    "class ModelEvaluationTrainingPipeline:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def main(self):\n",
    "        config = ConfigurationManager()\n",
    "        model_evaluation_config = config.get_model_evaluation_config()\n",
    "        model_evaluation = ModelEvaluation(config=model_evaluation_config)\n",
    "        \n",
    "        # Evaluate model (will skip if metrics already exist)\n",
    "        evaluation_result = model_evaluation.evaluate()\n",
    "        \n",
    "        return evaluation_result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        logger.info(f\">>>>>> Stage {STAGE_NAME} started <<<<<<\")\n",
    "        obj = ModelEvaluationTrainingPipeline()\n",
    "        result = obj.main()\n",
    "        \n",
    "        if result['status'] == 'completed':\n",
    "            logger.info(f\" Evaluation completed successfully!\")\n",
    "            logger.info(f\" Accuracy: {result['accuracy']*100:.2f}%\")\n",
    "            logger.info(f\" Test samples: {result['test_samples']}\")\n",
    "            logger.info(f\" Metrics saved at: {result['metrics_file']}\")\n",
    "            logger.info(f\">>>>>> Stage {STAGE_NAME} completed <<<<<<\\n\\nx==========x\")\n",
    "        else:\n",
    "            logger.info(f\">>>>>> Stage {STAGE_NAME} skipped (evaluation already completed) <<<<<<\\n\\nx==========x\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.exception(e)\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
